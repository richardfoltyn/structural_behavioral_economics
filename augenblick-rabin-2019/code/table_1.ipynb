{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augenblick and Rabin, 2019, \"An Experiment on Time Preference and Misprediction in Unpleasant Tasks\", Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors:  \n",
    "\n",
    "- Massimiliano Pozzi (Bocconi University, pozzi.massimiliano@studbocconi.it)\n",
    "- Salvatore Nunnari (Bocconi University, salvatore.nunnari@unibocconi.it)\n",
    "- Adaptations by Richard Foltyn (NHH, richard.foltyn@nhh.no)\n",
    "\n",
    "#### Description:\n",
    "\n",
    "The code in this Jupyter notebook performs the aggregate estimates to replicate column 1 of Table 1\n",
    "\n",
    "This notebook was tested with the following packages versions (also see the `environment.yml` file):\n",
    "- Python 3.11, numpy 1.26, scipy 1.11, pandas 2.0, autograd 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "from autograd.scipy.stats import norm    \n",
    "import autograd.numpy as anp\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "import autograd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning and Data Preparation\n",
    "\n",
    "We import the dataset containing the choices of all 100 individuals who participated to the experiment. To guarantee consistency with the authors' results, we then construct the primary sample used for the aggregate estimates. This sample consists of 72 individuals whose individual parameter estimates converged in less than 200 iterations when using the authors' Stata algorithm. In particular, we run the `03MergeIndMLEAndConstructMainSample.do` file provided by the authors. This script creates a file named `ind_to_keep.csv` which contains the identifiers of the individuals to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two datasets and drop subjects whose individual estimates do not converge\n",
    "\n",
    "dt = pd.read_stata('../input/decisions_data.dta')    # full sample\n",
    "ind_keep = pd.read_csv('../input/ind_to_keep.csv')   # import csv with ID of subjects to keep \n",
    "\n",
    "# drop subjects whose IDs are not listed in the ind_keep dataframe (28 individuals)\n",
    "\n",
    "dt = dt[dt.wid.isin(ind_keep.wid_col1)] # this is the primary sample for the aggregate estimates (72 individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove observations when a bonus was offered and create the following dummy variables that will be useful for estimation: `pb` is equal to one if the subject completed 10 mandatory tasks on subject-day (this is used to estimate the projection bias parameter $\\alpha$); `ind_effort10` and `ind_effort110` are equal to one if, respectively, the subject completed 10 or 110 tasks (and they are used for the Tobit correction when computing the likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations when a bonus was offered and create dummy variables. \n",
    "\n",
    "dt = dt[dt.bonusoffered !=1].copy()   # remove observations when a bonus was offered\n",
    "dt['pb']= dt['workdone1']/10   # pb dummy variable. workdone1 can either be 10 or 0, so dividing the variable by 10 creates our dummy\n",
    "dt['ind_effort10']  = (dt['effort']==10).astype(int)   # ind_effort10 dummy\n",
    "dt['ind_effort110'] = (dt['effort']==110).astype(int)  # ind_effort110 dummy\n",
    "dt = dt.reset_index(drop=True) # correct the index. The index should go from 0 to 8048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert individual ID stored in wid to integer\n",
    "dt[\"wid\"] = dt[\"wid\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8049 entries, 0 to 8048\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   wid            8049 non-null   int64  \n",
      " 1   netdistance    8049 non-null   float32\n",
      " 2   wage           8049 non-null   float32\n",
      " 3   today          8049 non-null   float32\n",
      " 4   prediction     8049 non-null   float32\n",
      " 5   pb             8049 non-null   float64\n",
      " 6   effort         8049 non-null   float32\n",
      " 7   ind_effort10   8049 non-null   int64  \n",
      " 8   ind_effort110  8049 non-null   int64  \n",
      "dtypes: float32(5), float64(1), int64(3)\n",
      "memory usage: 408.9 KB\n"
     ]
    }
   ],
   "source": [
    "# Keep only variables relevant for estimation\n",
    "keep = ['wid', 'netdistance', 'wage', 'today', 'prediction', 'pb', 'effort', 'ind_effort10', 'ind_effort110']\n",
    "dt = dt[keep].copy()\n",
    "dt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model and the Likelihood (Section 3 in Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent needs to choose the optimal effort $e$ to solve a simple tradeoff problem between disutility of effort and consumption utility derived from the consequent payment. More specifically, the agent takes a decision at a time $k$ to complete a certain number of tasks at time $t\\geq k$ and to get paid a wage $w$ per task at time $T > k$. \n",
    "Assuming the agent discounts utility using quasi-hyperbolic discounting and has a convex cost function $C(e)$, the problem can be conveniently written as:\n",
    "\n",
    "$$ \\max_{{e}} \\; \\delta^{T-k}⋅(e⋅w)- \\frac{1}{\\beta^{I(k=t)}}⋅\\frac{1}{\\beta_h^{I(p=1)}}⋅\\delta^{t-k}⋅ \\frac{e^\\gamma}{\\phi⋅\\gamma} $$\n",
    "\n",
    "The last term is a two parameter power cost function, $I(k=t)$ is an indicator function equal to one if the decision occurs in the same period as the effort, and $I(p=1)$ is an indicator that the decision is a prediction, $\\beta_h$ is the perceived present bias parameter (that is, the agent's degree of awareness of his present bias), and $\\delta$ is the standard time discounting parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative of the maximization problem above with respect to effort yields the following first order condition:\n",
    "\n",
    "$$  e^*= \\left(\\frac{\\delta^{T-k}⋅\\phi⋅w}{\\frac{1}{\\beta^{I(k=t)}}⋅\\frac{1}{\\beta_h^{I(p=1)}}⋅\\delta^{t-k}} \\right)^{\\frac{1}{\\gamma-1}} $$\n",
    "\n",
    "This is the optimal effort level, or what we will call in the code the predicted choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model heterogeneity, the authors assume that the observed effort is distributed as the predicted effort plus an implementation error which is Gaussian with mean zero and standard deviation $\\sigma$, so that the likelihood of observing an effort decision $e_j$ in the data is equal to:\n",
    "\n",
    "$$ L(e_j)= \\phi \\left(\\frac{e^*_j-e_j}{\\sigma}\\right)$$\n",
    "\n",
    "where $\\phi$ is the pdf of a standard normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with corner solutions we apply a Tobit correction, so that the likelihood to maximize is:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "L^{tobit}(e_j) &=\n",
    "    \\underbrace{(1-I(e=10)-I(e=110))⋅\\phi \\left(\\frac{e^*_j-e_j}{\\sigma}\\right)}_{\\text{interior choice}} \\\\\n",
    "    &+ \\underbrace{I(e=10)⋅\\left(1- \\Phi \\left(\\frac{e_j^*-10}{\\sigma}\\right)\\right)}_{\\text{lower boundary}} \\\\\n",
    "    &+ \\underbrace{I(e=110)⋅ \\Phi \\left(\\frac{e_j^*-110}{\\sigma}\\right)}_{\\text{upper boundary}}\n",
    "\\end{aligned}    \n",
    "$$\n",
    "\n",
    "where $\\Phi(\\bullet)$ is the cdf of a standard normal, while $I(e=10)$ and $I(e=110)$ are the indicators `ind_effort10` and `ind_effort110` explained above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, to keep the code simple, in this notebook we call effort the number of tasks performed by the agent, that is, the number of tasks chosen by the agent (ranging between 0 and 100) plus the compulsory 10 tasks. In the paper, the authors call effort just the number of tasks chosen by the agent (and, thus, they add 10 tasks to get total effort). This explains the differences between the equations in this notebook and equation (7), (8) and (10) in Section 3 of the paper. \n",
    "\n",
    "Our goal is to minimize the negative of the sum of the logarithms of $L^{tobit}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negloglike(\n",
    "        params: anp.ndarray, \n",
    "        netdistance: anp.ndarray, \n",
    "        wage: anp.ndarray, \n",
    "        today: anp.ndarray, \n",
    "        prediction: anp.ndarray, \n",
    "        pb: anp.ndarray, \n",
    "        effort: anp.ndarray, \n",
    "        ind_effort10: anp.ndarray, \n",
    "        ind_effort110: anp.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the negative of the log likelihood of observing our data given the parameters of the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : np.ndarray\n",
    "        Vector of model parameters: beta, betahat, delta, gamma, alpha, sigma\n",
    "    netdistance : np.ndarray \n",
    "        Stores (T-k)-(t-k) = T-t, the difference between the payment date T and the work time t\n",
    "    wage : np.ndarray\n",
    "        Stores the amount paid per task in a certain session\n",
    "    today : np.ndarray \n",
    "        Dummy variable equal to one if the decision involves the choice of work today\n",
    "    prediction : np.ndarray\n",
    "        Dummy variable equal to one if the decision involves the choice of work in the future\n",
    "    pb : np.ndarray\n",
    "        Dummy equal to one if the subject completed 10 mandatory tasks on subject-day \n",
    "    effort : np.ndarray\n",
    "        Number of tasks completed by a subject in a session. It can range from a minimum of 10 to a maximum of 110\n",
    "    ind_effort10 : np.ndarray \n",
    "        Dummy equal to one if the subject's effort was equal to 10\n",
    "    ind_effort110 : np.ndarray \n",
    "        Dummy equal to one if the subject's effort was equal to 110\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Value of negative log likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    beta, betahat, delta, gamma, phi, alpha, sigma = params\n",
    "\n",
    "    # predchoice is the predicted choice coming from the optimality condition of the subject\n",
    "    \n",
    "    predchoice=((phi*(delta**netdistance)*(beta**today)*(betahat**prediction)*wage)**(1/(gamma-1)))-pb*alpha\n",
    "    \n",
    "    # prob is a 1x8049 vector containing the probability of observing the effort of an individual. \n",
    "    # If effort is 10 or 110 we apply a Tobit correction\n",
    "    prob = (1-ind_effort10-ind_effort110)*norm.pdf(effort, predchoice, sigma) \\\n",
    "        +ind_effort10*(1 - norm.cdf((predchoice-effort)/sigma)) \\\n",
    "        +ind_effort110*norm.cdf((predchoice-effort)/sigma)\n",
    "            \n",
    "    # Add a small value close to zero if prob=0 or subtract a small value close to zero if prob=1. \n",
    "    # This is necessary to avoid problems when taking logs\n",
    "        \n",
    "    prob = anp.where(prob == 0.0, 1.0e-4, prob)\n",
    "    prob = anp.where(prob == 1.0, 1.0 - 1.0e-4, prob)\n",
    "    \n",
    "    negll = - anp.sum(anp.log(prob)) # negative log likelihood\n",
    "    \n",
    "    return negll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimation\n",
    "\n",
    "### Point Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now estimate the model. First, we need to initialize a vector with the starting parameters for the minimization algorithm. We then minimize the negative log-likelihood function using the scipy.optimize package and the Nelder-Mead algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial guesses (same as the ones used by the authors in their do.file) \n",
    "# and the arguments for the function to minimize\n",
    "\n",
    "# starting parameters for the algorithm\n",
    "\n",
    "beta_init = 0.8         # Present-bias discounting\n",
    "betahat_init = 1.0      # Perceived discounting\n",
    "delta_init = 1.0        # Discount factor\n",
    "gamma_init = 2.0        # Cost curvature\n",
    "phi_init = 500.0        # Preference parameter\n",
    "alpha_init = 7.0        # Projected task reduction\n",
    "sigma_init = 40.0       # Variance of Gaussian noise\n",
    "\n",
    "par_init = anp.array([beta_init, betahat_init, delta_init, gamma_init, phi_init, alpha_init, sigma_init])\n",
    "\n",
    "# Additional arguments: split DataFrame columns into individual arrays. \n",
    "# Order matters!\n",
    "columns = ['netdistance', 'wage', 'today', 'prediction', 'pb', 'effort', 'ind_effort10', 'ind_effort110']\n",
    "mle_args = tuple(anp.array(dt[name].to_numpy()) for name in columns)\n",
    "\n",
    "# we now find the estimates using the scipy.optimize package\n",
    "\n",
    "sol = opt.minimize(negloglike, par_init, args=mle_args, method='Nelder-Mead', options={'maxiter': 1500})\n",
    "res = sol.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present Bias β             0.834996\n",
      "Naive Pres. Bias β_h       0.999058\n",
      "Discount Factor δ          1.002621\n",
      "Cost Curvature γ           2.145292\n",
      "Cost Slope ϕ             724.145634\n",
      "Proj Task Reduction α      7.303662\n",
      "Sd of error term σ        42.621433\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print estimated parameters\n",
    "parameters_name = [\n",
    "    \"Present Bias β\",\n",
    "    \"Naive Pres. Bias β_h\",\n",
    "    \"Discount Factor δ\",\n",
    "    \"Cost Curvature γ\",\n",
    "    \"Cost Slope ϕ\",\n",
    "    \"Proj Task Reduction α\",\n",
    "    \"Sd of error term σ\"\n",
    "]\n",
    "\n",
    "print(pd.Series(res, index=parameters_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Errors\n",
    "\n",
    "We now estimate individual cluster robust standard errors. \n",
    "\n",
    "These are computed by taking the square root of the diagonal elements of the following matrix: \n",
    "\n",
    "$$ Adj⋅(H^{-1} \\cdot G \\cdot H^{-1}) $$ \n",
    "\n",
    "Where Adj is an adjustment for the degree of freedoms and the number of clusters:\n",
    "\n",
    "$$ Adj = \\frac{Nr.observations-1}{Nr.observations-Nr.parameters}⋅\\frac{Nr.clusters}{Nr.clusters-1} $$ \n",
    "\n",
    "$H^{-1}$ is the inverse of the Hessian of the negative log-likelihood evaluated in the minimum (our estimates), and $G$ is a $5\\times 5$ matrix of gradient contributions. \n",
    "\n",
    "We denote the gradient of the log likelihood function for a generic individual $i$ as follows:\n",
    "\n",
    "$$  g_i(y|\\theta) = [log f_i(y|\\theta)]' = \\frac{\\partial}{\\partial \\theta} log f_i(y|\\theta) $$\n",
    "\n",
    "where $\\theta$ is the parameters vector and $f_i(y|\\theta)$ is the likelihood function. Then $G$ is defined as follows:\n",
    "\n",
    "$$ G = \\sum_j \\left[\\sum_{i \\in c_j}g_i(y|\\hat{\\theta})\\right]^{\\top}\\left[\\sum_{i \\in c_j}g_i(y|\\hat{\\theta})\\right] $$\n",
    "\n",
    "where $J$ is the number of clusters (in our case the number of unique individuals = 72) and $c_j$ is a generic cluster $j$, that includes all observations for a specific individual (in our case 130). For more information on how to compute standard errors when using maximum likelihood, we refer the reader to David A. Freedman, 2006, [\"On The So-Called 'Huber Sandwich Estimator' and 'Robust Standard Errors'\"](https://snunnari.github.io/freedman.pdf), *The American Statistician*, 60:4, 299-302)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gradient and Hessian functions using autograd\n",
    "Hfun = autograd.hessian(negloglike)\n",
    "gradfun = autograd.grad(negloglike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Hessian\n",
    "hessian = Hfun(res, *mle_args)      # hessian\n",
    "hess_inv = np.linalg.inv(hessian)   # inverse of the hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient contribution for each individual \n",
    "# (using apply along axis=1)\n",
    "\n",
    "gradients = dt.apply(\n",
    "    lambda x: gradfun(res, *tuple(x[name] for name in columns)), \n",
    "    axis=1,\n",
    "    result_type='expand'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add individual ID, sum within individual\n",
    "gradients.index = pd.Index(dt[\"wid\"])\n",
    "gradients_indiv = gradients.groupby(\"wid\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute outer product for each individual\n",
    "gradients_indiv = gradients_indiv.to_numpy()\n",
    "# This creates an Nindiv x 5 x 5 array\n",
    "G_j = gradients_indiv[:, :, None] * gradients_indiv[:, None, :]\n",
    "\n",
    "# Sum over individuals to get final 5 x 5 matrix\n",
    "G = np.sum(G_j, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cluster robust standard errors\n",
    "Nobs = len(dt)\n",
    "Nindiv = len(dt['wid'].unique())\n",
    "# Number of parameters\n",
    "K = len(res)\n",
    "\n",
    "# Compute the adjustment for degree of freedoms and number of clusters\n",
    "adj = (Nobs-1)/(Nobs-K) * Nindiv/(Nindiv-1)\n",
    "\n",
    "varcov_estimates = adj *(hess_inv @ G @ hess_inv)   # var-cov matrix of our estimates\n",
    "se_cluster = np.sqrt(np.diag((varcov_estimates)))   # individual cluster robust standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "We now do some hypothesis testing on the parameters we obtained. We compute the $z$-test statistics and the corresponding $p$-values to check if $\\beta$, $\\beta_h$ or $\\delta$ are statistically different from one. We then compute the $p$-value of a $z$-test to check if the parameter for projection bias is statistically different from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the z-test statistics and the corresponding p-values to check if beta, betahat, delta are statistically different from one\n",
    "\n",
    "zvalues_1 = (np.array(res[0:3])-1)/np.array(se_cluster[0:3]) # the first three elements are for beta (position 0), betahat (position 1) and delta (position 2)\n",
    "pvalues_1 = 2*(1-norm.cdf(np.abs(zvalues_1),0,1))\n",
    "\n",
    "# Now compute the z-test statistics and the corresponding p-value for H0: alpha different from 0\n",
    "\n",
    "zvalue_a = res[5]/se_cluster[5]\n",
    "pvalue_a = 2*(1-norm.cdf(np.abs(zvalue_a),0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Print and Save Estimation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a table with point estimates and individual cluster robust standard errors. We then save the results as a csv file in the output folder and print the results. This replicates Column 1 of Table 1 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the results and save it as a csv file in output. We round the results up to the 3rd decimal.\n",
    "\n",
    "Table_1 = pd.DataFrame({\n",
    "    'parameters': parameters_name,\n",
    "    'estimates': np.round(res, 3),\n",
    "    'standarderr': np.round(se_cluster,3)\n",
    "})\n",
    "\n",
    "Table_1.to_csv('../output/table1_python.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Primary aggregate structural estimation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>estimates</th>\n",
       "      <th>standarderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Present Bias β</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Pres. Bias β_h</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discount Factor δ</td>\n",
       "      <td>1.003</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cost Curvature γ</td>\n",
       "      <td>2.145</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cost Slope ϕ</td>\n",
       "      <td>724.146</td>\n",
       "      <td>255.711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Proj Task Reduction α</td>\n",
       "      <td>7.304</td>\n",
       "      <td>2.599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sd of error term σ</td>\n",
       "      <td>42.621</td>\n",
       "      <td>3.305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              parameters  estimates  standarderr\n",
       "0         Present Bias β      0.835        0.038\n",
       "1   Naive Pres. Bias β_h      0.999        0.011\n",
       "2      Discount Factor δ      1.003        0.003\n",
       "3       Cost Curvature γ      2.145        0.071\n",
       "4           Cost Slope ϕ    724.146      255.711\n",
       "5  Proj Task Reduction α      7.304        2.599\n",
       "6     Sd of error term σ     42.621        3.305"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 8,049\n",
      "Number of participants: 72\n",
      "Log Likelihood: -28,412\n",
      "H_0(β=1) 0.00\n",
      "H_0(β_h=1): 0.93\n",
      "H_0(α=0): 0.005\n",
      "H_0(δ=1): 0.37\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"Table 1: Primary aggregate structural estimation\")\n",
    "display(Table_1)\n",
    "print(\"Number of observations:\", f\"{Nobs:,}\")\n",
    "print(\"Number of participants:\", f\"{Nindiv:,}\")\n",
    "print(\"Log Likelihood:\", f\"{-sol.fun:,.0f}\")\n",
    "print(\"H_0(β=1)\", f\"{pvalues_1[0]:,.2f}\")\n",
    "print(\"H_0(β_h=1):\", f\"{pvalues_1[1]:,.2f}\")\n",
    "print(\"H_0(α=0):\", f\"{pvalue_a:,.3f}\")\n",
    "print(\"H_0(δ=1):\", f\"{pvalues_1[2]:,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
